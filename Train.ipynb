{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1e8R3NsYKeM6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Getting Dataset\n","from urllib.request import urlretrieve\n","data_set_url = 'http://images.cocodataset.org/zips/train2014.zip'\n","urlretrieve(data_set_url,'/content/data_set.zip')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y3iDFs_czk7C","colab_type":"code","colab":{}},"cell_type":"code","source":["# Extract dataset folder\n","import zipfile\n","zip_ref = zipfile.ZipFile('/content/data_set.zip','r')\n","zip_ref.extractall('/content/training_data')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ydKAVNCdxLoP","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/my_drive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P4Mr6coi4NIp","colab_type":"code","outputId":"5cf151da-03c5-4311-b234-c8a8a034e425","executionInfo":{"status":"ok","timestamp":1549131573910,"user_tz":-120,"elapsed":1772,"user":{"displayName":"Dot AI","photoUrl":"","userId":"16683311460537221843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Import libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from keras import layers\n","from keras import backend as K\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from keras.engine.topology import Layer\n","from keras.regularizers import Regularizer\n","from keras.layers.normalization import BatchNormalization\n","from keras.preprocessing.image import array_to_img, load_img, img_to_array\n","from matplotlib import pyplot as plt\n","from IPython.display import clear_output"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"prv8q_RH83ee","colab_type":"code","colab":{}},"cell_type":"code","source":["img_width,img_height = (512,512)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9oy8CFON8Ym1","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_image(image_path,rows,cols):\n","    img = load_img(image_path, target_size=(rows, cols))\n","    img = img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    return img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4vbZ8KUTs-OW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Layer used to rescale the output values from [-1,1] to [0,255]\n","class Denormalize(Layer):\n","    def __init__(self, **kwargs):\n","        super(Denormalize, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        pass\n","\n","    def call(self, x, mask=None):\n","        return (x + 1) * 127.\n","\n","#---------------------------------------------------------------------------------------------\n","# Layer used to normalize input image pixels by its values by 255.0\n","class InputNormalize(Layer):\n","    def __init__(self, **kwargs):\n","        super(InputNormalize, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        pass\n","\n","#     def compute_output_shape(self,input_shape):\n","#         return input_shape\n","\n","    def call(self, x, mask=None):\n","        return x/255.\n","\n","#---------------------------------------------------------------------------------------------      \n","# Layer used to normailze the VGG16 model input\n","class VGGNormalize(Layer):\n","    def __init__(self, **kwargs):\n","        super(VGGNormalize, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        pass\n","\n","    def call(self, x, mask=None):\n","        x = preprocess_input(x)          \n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-NPRrOcousKj","colab_type":"code","colab":{}},"cell_type":"code","source":["# Transformation Network\n","def transform_net(img_width,img_height):\n","  input_tensor = layers.Input(shape=(img_width,img_height,3))\n","  input_tensor2 = InputNormalize()(input_tensor)\n","  \n","  x = layers.Conv2D(32, kernel_size = (9,9), strides = (1,1), padding = 'same')(input_tensor2)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  x = layers.Conv2D(64, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  x = layers.Conv2D(128, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  x = residual_block(x)\n","  x = residual_block(x)\n","  x = residual_block(x)\n","  x = residual_block(x)\n","  x = residual_block(x)\n","\n","  x = layers.Conv2DTranspose(64, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  \n","  x = layers.Conv2DTranspose(32, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  x = layers.Conv2DTranspose(3, kernel_size = (9,9), strides = (1,1), padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  output_tensor = layers.Activation('tanh')(x)  \n","  \n","  output_tensor2 = Denormalize()(output_tensor)\n","  model = Model(inputs = input_tensor,outputs = output_tensor2)  \n","  return model  \n","\n","def residual_block(x):\n","  y = x\n","  x = layers.Conv2D(128,kernel_size = (3,3),strides = (1,1),padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('relu')(x)\n","  \n","  x = layers.Conv2D(128,kernel_size = (3,3),strides = (1,1),padding = 'same')(x)\n","  x = layers.BatchNormalization()(x)\n","  res = layers.merge.add([x, y])\n","  return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yry_mJTK4J8x","colab_type":"code","colab":{}},"cell_type":"code","source":["def dummy_loss(y_true, y_pred ):\n","    return K.variable(0.0)\n","\n","def gram_matrix(x):\n","    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n","\n","    shape = K.shape(x)\n","    \n","    C, W, H = (shape[0],shape[1], shape[2])\n","    \n","#   Reshape the features to (C X H*W) \n","    cf = K.reshape(features ,(C,-1))\n","    \n","    gram = K.dot(cf, K.transpose(cf)) /  K.cast(C*W*H,dtype='float32')\n","    return gram\n","\n","#---------------------------------------------------------------------------------------------              \n","class StyleReconstructionRegularizer(Regularizer):\n","    def __init__(self, style_feature_target, weight=1.0):\n","        self.style_feature_target = style_feature_target\n","        self.weight = weight\n","        self.uses_learning_phase = False\n","        super(StyleReconstructionRegularizer, self).__init__()\n","\n","        self.style_gram = gram_matrix(style_feature_target)\n","\n","    def __call__(self, x):\n","        output = x.output[0] # Generated by the transformation network\n","        \n","        loss = self.weight *  K.sum(K.mean(K.square(self.style_gram-gram_matrix(output))))\n","        return loss\n","\n","#---------------------------------------------------------------------------------------------\n","class FeatureReconstructionRegularizer(Regularizer):\n","    def __init__(self, weight=1.0):\n","        self.weight = weight\n","        self.uses_learning_phase = False\n","        super(FeatureReconstructionRegularizer, self).__init__()\n","\n","    def __call__(self, x):\n","        generated = x.output[0] # Generated by the transformation network\n","        content = x.output[1]   # The original input image\n","\n","        loss = self.weight *  K.sum(K.mean(K.square(content-generated)))\n","\n","        return loss\n","      \n","#---------------------------------------------------------------------------------------------\n","class TVRegularizer(Regularizer):\n","    def __init__(self, weight=1.0):\n","        self.weight = weight\n","        self.uses_learning_phase = False\n","        super(TVRegularizer, self).__init__()\n","\n","    def __call__(self, x):\n","        x_out = x.output\n","        \n","        shape = K.shape(x_out)\n","        img_width, img_height,channel = (shape[1],shape[2], shape[3])\n","\n","        a = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, 1:, :img_height - 1, :])\n","        b = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, :img_width - 1, 1:, :])\n","\n","        loss = self.weight * K.sum(K.pow(a + b, 1.25)) \n","        return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3PcXDLz-3Lz3","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_style_loss(vgg,style_image_path,vgg_layers,vgg_output_dict,img_width, img_height,weight):\n","    style_img = preprocess_image(style_image_path, img_width, img_height)\n","    print('Getting style features from VGG network.')\n","    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1','block5_conv1']\n","\n","    style_layer_outputs = []\n","\n","    for layer in style_layers:\n","        style_layer_outputs.append(vgg_output_dict[layer])\n","\n","#   Getting the output of each layer of the style layers \n","    vgg_style_func = K.function([vgg.layers[-19].input], style_layer_outputs)\n","    style_features = vgg_style_func([style_img])\n","    \n","#   Adding the style losses to multiple layers\n","    for i, layer_name in enumerate(style_layers):\n","        layer = vgg_layers[layer_name]\n","\n","        feature_var = K.variable(value=style_features[i][0])\n","        style_loss = StyleReconstructionRegularizer(\n","                            style_feature_target=feature_var,\n","                            weight=weight)(layer)\n","        \n","        layer.add_loss(style_loss)\n","        \n","#---------------------------------------------------------------------------------------------        \n","def add_content_loss(vgg_layers,vgg_output_dict,weight):\n","    content_layer = 'block4_conv2'\n","\n","    layer = vgg_layers[content_layer]\n","    content_regularizer = FeatureReconstructionRegularizer(weight)(layer)\n","    layer.add_loss(content_regularizer)\n","    \n","#---------------------------------------------------------------------------------------------        \n","def add_total_variation_loss(transform_output_layer,weight):\n","    # Total Variation Regularization\n","    layer = transform_output_layer  # Transformation output layer\n","    tv_regularizer = TVRegularizer(weight)(layer)\n","    layer.add_loss(tv_regularizer)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Vjn6HqJEzRC5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Declare and initialize the model for training phase\n","trans = transform_net(img_width,img_height)\n","tensor1 = layers.merge.concatenate([trans.output,trans.input],axis = 0)\n","tensor2 = VGGNormalize(name=\"vgg_normalize\")(tensor1)\n","vgg1 = VGG16(include_top = False,input_tensor = tensor2,weights = None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6V3MWxWPnkWL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading the VGG16 weights\n","vgg1.load_weights(filepath ='/content/my_drive/My Drive/Project/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kIEGpq0VocVy","colab_type":"code","colab":{}},"cell_type":"code","source":["trans.load_weights('/content/my_drive/My Drive/Project/style_weights/picasso/epoch1/picasso_weights_after_40000iteration_1st_epoch.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dT4dXKNv6XnE","colab_type":"code","outputId":"7108cd08-b837-4af4-84bf-26d52e2c058e","executionInfo":{"status":"ok","timestamp":1549131593875,"user_tz":-120,"elapsed":6664,"user":{"displayName":"Dot AI","photoUrl":"","userId":"16683311460537221843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Select the path of the style image\n","style_path = '/content/my_drive/My Drive/Project/style_weights/picasso/picasso.jpg'\n","vgg_output_dict = dict([(layer.name, layer.output) for layer in vgg1.layers[-18:]])\n","vgg_layers = dict([(layer.name, layer) for layer in vgg1.layers[-18:]])\n","add_style_loss(vgg1, style_path, vgg_layers, vgg_output_dict, img_width, img_height,3.0)\n","add_content_loss(vgg_layers,vgg_output_dict,2.0)\n","add_total_variation_loss(trans.layers[-1],5e-5)\n","\n","# Freeze all VGG16 layers\n","for layer in vgg1.layers[-19:]:\n","  layer.trainable = False\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Getting style features from VGG network.\n"],"name":"stdout"}]},{"metadata":{"id":"6pBVD5MYCECm","colab_type":"code","colab":{}},"cell_type":"code","source":["# Select the optimizer for the model\n","from keras.optimizers import Adam\n","optimizer = Adam()\n","vgg1.compile(optimizer, loss = dummy_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kOD-lPgMkIcw","colab_type":"code","colab":{}},"cell_type":"code","source":["files = [f for f in os.listdir('/content/training_data/train2014')]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PdJ-qouuwn2k","colab_type":"code","colab":{}},"cell_type":"code","source":["# Trainig the model\n","y = np.zeros((1,img_width,img_height,3),dtype='float32')\n","j = 0\n","history = []\n","for i in range(j,82783):\n","  v = preprocess_image('/content/training_data/train2014/{}'.format(files[i]),img_width,img_height)\n","  h = vgg1.train_on_batch(v,y)\n","  history.append(h)\n","  if i % 100 == 0:\n","    clear_output()\n","    print('loss: {} at iteration: {} '.format(h,i))\n","    res = trans.predict(v)\n","    resImg = array_to_img(res[0])\n","    plt.imshow(resImg)\n","    plt.figure()\n","    inputImg = array_to_img(v[0])\n","    plt.imshow(inputImg)\n","    plt.show()\n","    \n","  if (i % 10000 == 0 and i > 0) or i == 82782:\n","    trans.save_weights('/content/my_drive/My Drive/Project/style_weights/picasso/epoch1/picasso_weights_after_{}iteration_1st_epoch.h5'.format(i))\n","    inputImg.save('/content/my_drive/My Drive/Project/style_weights/picasso/epoch1/inputImage_after_{}iteration_1st_epoch.jpg'.format(i))\n","    resImg.save('/content/my_drive/My Drive/Project/style_weights/picasso/epoch1/outputImage_after_{}iteration_1st_epoch.jpg'.format(i))\n","    pd.DataFrame(history).to_csv('/content/my_drive/My Drive/Project/style_weights/picasso/epoch1/picasso_history_of_losses_after_{}iteration_1st_epoch.csv'.format(i))    \n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"pPVykCOnfxTa","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}